###distance
R01=as.matrix(abs(outer(input1, input1, "-")))
R02=as.matrix(abs(outer(input2, input2, "-")))
##initial value
param_ini=c(-2,-2,-3)
Neg_log_lik_eigen_with_nugget(param_ini)
##add derivative may make it faster and more stable, but let's use numerical gradient for now
# m_eigen=try(optim(param_ini,method ="L-BFGS-B",#lower = c(-Inf,-Inf,-Inf),
#                   Neg_log_lik_eigen_with_nugget),silent=T) ##it will be more accurate to use derivative
# while(!is.numeric(m_eigen[[1]])){
#   m_eigen=try(optim(param_ini+runif(3),method ="L-BFGS-B",#lower = c(-Inf,-Inf,-Inf),
#                     Neg_log_lik_eigen_with_nugget),silent=T) ##it will be more accurate to use derivative
#
# }
#
#
#
# ##build predictive mean
#
# beta= exp(m_eigen$par[1:p])
# nu=exp(m_eigen$par[p+1])
beta = c(0.3, 0.4) #beta small
nu = 1 # nu large
#R1=Exp_funct(R01, beta=beta[1])
#R2=Exp_funct(R02, beta=beta[2])
R1=Matern_5_2_funct(R01, beta=beta[1])
R2=Matern_5_2_funct(R02, beta=beta[2])
eigen_R1=eigen(R1)
eigen_R2=eigen(R2)
###get theta
U_x=matrix(NA,N,q_X)
for(i_q in 1:q_X){
U_x=as.vector(t(eigen_R1$vectors)%*% X_list[[i_q]]%*%eigen_R2$vectors)
}
Lambda_tilde_inv=1/(kronecker(eigen_R2$values,eigen_R1$values, FUN = "*")+nu)
#eigen(R_tilde.inv)$values-sort(Lambda_tilde_inv,decreasing=T)
Lambda_tilde_inv_U_x= Lambda_tilde_inv*U_x
X_R_tilde_inv_X_inv=solve(t(U_x)%*%(Lambda_tilde_inv_U_x))
output_tilde=as.vector(t(eigen_R1$vectors)%*% output_mat%*%eigen_R2$vectors)
theta_hat=X_R_tilde_inv_X_inv%*%(t(Lambda_tilde_inv_U_x)%*%output_tilde)
output_mat_normalized=matrix(as.vector(output_mat)-X%*%theta_hat,n1,n2)
### test on input,it will be faster if we use filling
testing_input=input
testing_input1=input1
testing_input2=input2
X_testing=X
r01=abs(outer(input1,testing_input1,'-'))
r02=abs(outer(input2,testing_input2,'-'))
r1=Matern_5_2_funct(r01, beta[1])
r2=Matern_5_2_funct(r02, beta[2])
output_normalize_tilde=as.vector(t(eigen_R1$vectors)%*% output_mat_normalized%*%eigen_R2$vectors)
output_normalized_tilde_lambda_inv_mat=matrix(Lambda_tilde_inv*output_normalize_tilde,n1,n2)
R_tilde_inv_output_normalize=as.vector((eigen_R1$vectors)%*% output_normalized_tilde_lambda_inv_mat%*%t(eigen_R2$vectors))
R_tilde_inv_output_normalize_mat=matrix(R_tilde_inv_output_normalize,n1,n2)
predmean=X_testing%*%theta_hat+as.vector(t(r1)%*%R_tilde_inv_output_normalize_mat%*%r2)
predmean_mat=matrix(predmean,n1,n2)
z_lim=c(min(output_mat,predmean_mat),max(output_mat,predmean_mat) )
#image2D(predmean_mat,x=input1,y=input2,zlim=z_lim,main='GP prediction')
###build gradient estimate; use KF will save some memory
###here for saving the memory we use some same names
delta1=10^{-5}
delta2=10^{-5}
##plus for 1
testing_input1_delta=input1+rep(delta1,n1) #t(matrix(c(delta1,0),2,N))
testing_input2_delta=input2 #t(matrix(c(delta1,0),2,N))
r01_delta=abs(outer(input1,testing_input1_delta,'-'))
r02_delta=abs(outer(input2,testing_input2_delta,'-'))
r1_delta=Matern_5_2_funct(r01_delta, beta[1])
r2_delta=Matern_5_2_funct(r02_delta, beta[2])
predmean_plus_1=X_testing%*%theta_hat+as.vector(t(r1_delta)%*%R_tilde_inv_output_normalize_mat%*%r2_delta)
##minus for 1
testing_input1_delta=input1-rep(delta1,n1) #t(matrix(c(delta1,0),2,N))
testing_input2_delta=input2 #t(matrix(c(delta1,0),2,N))
r01_delta=abs(outer(input1,testing_input1_delta,'-'))
r02_delta=abs(outer(input2,testing_input2_delta,'-'))
r1_delta=Matern_5_2_funct(r01_delta, beta[1])
r2_delta=Matern_5_2_funct(r02_delta, beta[2])
predmean_minus_1=X_testing%*%theta_hat+as.vector(t(r1_delta)%*%R_tilde_inv_output_normalize_mat%*%r2_delta)
predmean_numer_grad1=(predmean_plus_1-predmean_minus_1)/(2*delta1)
predmean_numer_grad1_mat=matrix(predmean_numer_grad1,n1,n2)
#image2D(predmean_numer_grad1_mat)
##build grad for 2
##plus for 2
testing_input1_delta=input1 #t(matrix(c(delta1,0),2,N))
testing_input2_delta=input2+rep(delta2,n2) #t(matrix(c(delta1,0),2,N))
r01_delta=abs(outer(input1,testing_input1_delta,'-'))
r02_delta=abs(outer(input2,testing_input2_delta,'-'))
r1_delta=Matern_5_2_funct(r01_delta, beta[1])
r2_delta=Matern_5_2_funct(r02_delta, beta[2])
predmean_plus_2=X_testing%*%theta_hat+as.vector(t(r1_delta)%*%R_tilde_inv_output_normalize_mat%*%r2_delta)
##minus for 2
testing_input1_delta=input1 #t(matrix(c(delta1,0),2,N))
testing_input2_delta=input2-rep(delta2,n2) #t(matrix(c(delta1,0),2,N))
r01_delta=abs(outer(input1,testing_input1_delta,'-'))
r02_delta=abs(outer(input2,testing_input2_delta,'-'))
r1_delta=Matern_5_2_funct(r01_delta, beta[1])
r2_delta=Matern_5_2_funct(r02_delta, beta[2])
predmean_minus_2=X_testing%*%theta_hat+as.vector(t(r1_delta)%*%R_tilde_inv_output_normalize_mat%*%r2_delta)
predmean_numer_grad2=(predmean_plus_2-predmean_minus_2)/(2*delta2)
predmean_numer_grad2_mat=matrix(predmean_numer_grad2,n1,n2)
#image2D(predmean_numer_grad2_mat)
predmean_numer_grad_magnitude_mat=sqrt(predmean_numer_grad1_mat^2+predmean_numer_grad2_mat^2)
#image2D(predmean_numer_grad_magnitude_mat)
list(
predmean_mat = predmean_mat,
grad1 = predmean_numer_grad1_mat,
grad2 = predmean_numer_grad2_mat,
grad_magnitude = predmean_numer_grad_magnitude_mat,
param = c(beta,nu)
)
}
GP_info <- demo_GP_mod(demo_img)
par(mfrow = c(1, 3))
image2D(demo_img, main = "Original Img")
image2D(GP_info$predmean_mat, main = "GP predmean")
image2D(demo_img - GP_info$predmean_mat, main = "Difference")
demo_GP_mod <- function(output_mat){
n1=dim(output_mat)[1]
n2=dim(output_mat)[2]
N=n1*n2
p=2 ##2D input
set.seed(1)
input1=as.numeric(seq(0,1,1/(n1-1)))
input2=as.numeric(seq(0,1,1/(n2-1)))
input=cbind(rep(input1,n2),as.vector(t(matrix(input2,n2,n1))))
gradient_matrix <- gradient(as.matrix(output_mat))
##the range here
range=c(1,1)
##1.numerical gradient estimation
h1=range[1]/(n1-1)
h2=range[2]/(n2-1)
output_numer_grad1=matrix(NA,n1,n2)
output_numer_grad2=matrix(NA,n1,n2)
output_numer_grad1[1,]=(output_mat[2,]-output_mat[1,])/h1
output_numer_grad1[n1,]=(output_mat[n1,]-output_mat[n1-1,])/h1
output_numer_grad1[2:(n1-1),]=(output_mat[3:(n1),]-output_mat[1:(n1-2),])/(2*h1)
output_numer_grad2[,1]=(output_mat[,2]-output_mat[,1])/h2
output_numer_grad2[,n2]=(output_mat[,n2]-output_mat[,n2-1])/h2
output_numer_grad2[,2:(n2-1)]=(output_mat[,3:n2]-output_mat[,1:(n2-2)])/(2*h2)
output_numer_grad_magnitude=sqrt(output_numer_grad1^2+output_numer_grad2^2)
# image2D(output_numer_grad_magnitude)
##2.use numerical grad of post mean of separate model
Matern_5_2_funct<-function(d,beta){
x=sqrt(5)*beta*d
(1+x+x^2/3)*exp(-x)
}
# Exp_funct<-function(d,beta){
#   x=beta*d
#   exp(-x)
# }
Neg_log_lik_eigen_with_nugget <- function(param) {
##do not confuse this beta
beta= exp(param[1:p])
nu=exp(param[p+1])
#R1=Exp_funct(R01, beta=beta[1])
#R2=Exp_funct(R02, beta=beta[2])
R1=Matern_5_2_funct(R01, beta=beta[1])
R2=Matern_5_2_funct(R02, beta=beta[2])
eigen_R1=eigen(R1)
eigen_R2=eigen(R2)
U_x=matrix(NA,N,q_X)
for(i_q in 1:q_X){
U_x=as.vector(t(eigen_R1$vectors)%*% X_list[[i_q]]%*%eigen_R2$vectors)
}
Lambda_tilde_inv=1/(kronecker(eigen_R2$values,eigen_R1$values, FUN = "*")+nu)
#eigen(R_tilde.inv)$values-sort(Lambda_tilde_inv,decreasing=T)
Lambda_tilde_inv_U_x= Lambda_tilde_inv*U_x
X_R_tilde_inv_X_inv=solve(t(U_x)%*%(Lambda_tilde_inv_U_x))
output_tilde=as.vector(t(eigen_R1$vectors)%*% output_mat%*%eigen_R2$vectors)
theta_hat=X_R_tilde_inv_X_inv%*%(t(Lambda_tilde_inv_U_x)%*%output_tilde)
output_mat_normalized=matrix(as.vector(output_mat)-X%*%theta_hat,n1,n2)
output_normalize_tilde=as.vector(t(eigen_R1$vectors)%*% output_mat_normalized%*%eigen_R2$vectors)
S_2=sum((output_normalize_tilde)*Lambda_tilde_inv*output_normalize_tilde  )
-(1/2*sum(log(Lambda_tilde_inv))-N/2*log(S_2))
}
##mean basis
X=matrix(1,N,1)  ##use constant mean basis
q_X=dim(X)[2]
X_list=as.list(1:q_X)
for(i_q in 1:q_X){
X_list[[i_q]]=matrix(X[,i_q],n1,n2)
}
###distance
R01=as.matrix(abs(outer(input1, input1, "-")))
R02=as.matrix(abs(outer(input2, input2, "-")))
##initial value
param_ini=c(-2,-2,-3)
Neg_log_lik_eigen_with_nugget(param_ini)
##add derivative may make it faster and more stable, but let's use numerical gradient for now
# m_eigen=try(optim(param_ini,method ="L-BFGS-B",#lower = c(-Inf,-Inf,-Inf),
#                   Neg_log_lik_eigen_with_nugget),silent=T) ##it will be more accurate to use derivative
# while(!is.numeric(m_eigen[[1]])){
#   m_eigen=try(optim(param_ini+runif(3),method ="L-BFGS-B",#lower = c(-Inf,-Inf,-Inf),
#                     Neg_log_lik_eigen_with_nugget),silent=T) ##it will be more accurate to use derivative
#
# }
#
#
#
# ##build predictive mean
#
# beta= exp(m_eigen$par[1:p])
# nu=exp(m_eigen$par[p+1])
beta = c(3, 4) #beta small
nu = 0.1 # nu large
#R1=Exp_funct(R01, beta=beta[1])
#R2=Exp_funct(R02, beta=beta[2])
R1=Matern_5_2_funct(R01, beta=beta[1])
R2=Matern_5_2_funct(R02, beta=beta[2])
eigen_R1=eigen(R1)
eigen_R2=eigen(R2)
###get theta
U_x=matrix(NA,N,q_X)
for(i_q in 1:q_X){
U_x=as.vector(t(eigen_R1$vectors)%*% X_list[[i_q]]%*%eigen_R2$vectors)
}
Lambda_tilde_inv=1/(kronecker(eigen_R2$values,eigen_R1$values, FUN = "*")+nu)
#eigen(R_tilde.inv)$values-sort(Lambda_tilde_inv,decreasing=T)
Lambda_tilde_inv_U_x= Lambda_tilde_inv*U_x
X_R_tilde_inv_X_inv=solve(t(U_x)%*%(Lambda_tilde_inv_U_x))
output_tilde=as.vector(t(eigen_R1$vectors)%*% output_mat%*%eigen_R2$vectors)
theta_hat=X_R_tilde_inv_X_inv%*%(t(Lambda_tilde_inv_U_x)%*%output_tilde)
output_mat_normalized=matrix(as.vector(output_mat)-X%*%theta_hat,n1,n2)
### test on input,it will be faster if we use filling
testing_input=input
testing_input1=input1
testing_input2=input2
X_testing=X
r01=abs(outer(input1,testing_input1,'-'))
r02=abs(outer(input2,testing_input2,'-'))
r1=Matern_5_2_funct(r01, beta[1])
r2=Matern_5_2_funct(r02, beta[2])
output_normalize_tilde=as.vector(t(eigen_R1$vectors)%*% output_mat_normalized%*%eigen_R2$vectors)
output_normalized_tilde_lambda_inv_mat=matrix(Lambda_tilde_inv*output_normalize_tilde,n1,n2)
R_tilde_inv_output_normalize=as.vector((eigen_R1$vectors)%*% output_normalized_tilde_lambda_inv_mat%*%t(eigen_R2$vectors))
R_tilde_inv_output_normalize_mat=matrix(R_tilde_inv_output_normalize,n1,n2)
predmean=X_testing%*%theta_hat+as.vector(t(r1)%*%R_tilde_inv_output_normalize_mat%*%r2)
predmean_mat=matrix(predmean,n1,n2)
z_lim=c(min(output_mat,predmean_mat),max(output_mat,predmean_mat) )
#image2D(predmean_mat,x=input1,y=input2,zlim=z_lim,main='GP prediction')
###build gradient estimate; use KF will save some memory
###here for saving the memory we use some same names
delta1=10^{-5}
delta2=10^{-5}
##plus for 1
testing_input1_delta=input1+rep(delta1,n1) #t(matrix(c(delta1,0),2,N))
testing_input2_delta=input2 #t(matrix(c(delta1,0),2,N))
r01_delta=abs(outer(input1,testing_input1_delta,'-'))
r02_delta=abs(outer(input2,testing_input2_delta,'-'))
r1_delta=Matern_5_2_funct(r01_delta, beta[1])
r2_delta=Matern_5_2_funct(r02_delta, beta[2])
predmean_plus_1=X_testing%*%theta_hat+as.vector(t(r1_delta)%*%R_tilde_inv_output_normalize_mat%*%r2_delta)
##minus for 1
testing_input1_delta=input1-rep(delta1,n1) #t(matrix(c(delta1,0),2,N))
testing_input2_delta=input2 #t(matrix(c(delta1,0),2,N))
r01_delta=abs(outer(input1,testing_input1_delta,'-'))
r02_delta=abs(outer(input2,testing_input2_delta,'-'))
r1_delta=Matern_5_2_funct(r01_delta, beta[1])
r2_delta=Matern_5_2_funct(r02_delta, beta[2])
predmean_minus_1=X_testing%*%theta_hat+as.vector(t(r1_delta)%*%R_tilde_inv_output_normalize_mat%*%r2_delta)
predmean_numer_grad1=(predmean_plus_1-predmean_minus_1)/(2*delta1)
predmean_numer_grad1_mat=matrix(predmean_numer_grad1,n1,n2)
#image2D(predmean_numer_grad1_mat)
##build grad for 2
##plus for 2
testing_input1_delta=input1 #t(matrix(c(delta1,0),2,N))
testing_input2_delta=input2+rep(delta2,n2) #t(matrix(c(delta1,0),2,N))
r01_delta=abs(outer(input1,testing_input1_delta,'-'))
r02_delta=abs(outer(input2,testing_input2_delta,'-'))
r1_delta=Matern_5_2_funct(r01_delta, beta[1])
r2_delta=Matern_5_2_funct(r02_delta, beta[2])
predmean_plus_2=X_testing%*%theta_hat+as.vector(t(r1_delta)%*%R_tilde_inv_output_normalize_mat%*%r2_delta)
##minus for 2
testing_input1_delta=input1 #t(matrix(c(delta1,0),2,N))
testing_input2_delta=input2-rep(delta2,n2) #t(matrix(c(delta1,0),2,N))
r01_delta=abs(outer(input1,testing_input1_delta,'-'))
r02_delta=abs(outer(input2,testing_input2_delta,'-'))
r1_delta=Matern_5_2_funct(r01_delta, beta[1])
r2_delta=Matern_5_2_funct(r02_delta, beta[2])
predmean_minus_2=X_testing%*%theta_hat+as.vector(t(r1_delta)%*%R_tilde_inv_output_normalize_mat%*%r2_delta)
predmean_numer_grad2=(predmean_plus_2-predmean_minus_2)/(2*delta2)
predmean_numer_grad2_mat=matrix(predmean_numer_grad2,n1,n2)
#image2D(predmean_numer_grad2_mat)
predmean_numer_grad_magnitude_mat=sqrt(predmean_numer_grad1_mat^2+predmean_numer_grad2_mat^2)
#image2D(predmean_numer_grad_magnitude_mat)
list(
predmean_mat = predmean_mat,
grad1 = predmean_numer_grad1_mat,
grad2 = predmean_numer_grad2_mat,
grad_magnitude = predmean_numer_grad_magnitude_mat,
param = c(beta,nu)
)
}
GP_info <- demo_GP_mod(demo_img)
par(mfrow = c(1, 3))
image2D(demo_img, main = "Original Img")
image2D(GP_info$predmean_mat, main = "GP predmean")
image2D(demo_img - GP_info$predmean_mat, main = "Difference")
x <- seq(0, 10, by = 0.1)
d <- dgamma(x, shape = 1, rate = 1)
plot(x, d, type = "l", main = "Gamma Density")
x <- seq(0, 10, by = 0.1)
d <- dgamma(x, shape = 8, rate = 1)
plot(x, d, type = "l", main = "Gamma Density")
final_results <- read.csv("final_results.csv")
setwd("~/GitHub/RStan_prosodic")
final_results <- read.csv("final_results.csv")
load("sbc.Rdata")
display_images("an")
library(png)
library(grid)
display_images <- function(word) {
# Construct the path to the subfolder
folder_path <- file.path("Figures", word)
# Get the list of png files in the subfolder
image_files <- list.files(folder_path, pattern = "\\.png$", full.names = TRUE)
# Get the base names of the image files for titles
image_titles <- list.files(folder_path, pattern = "\\.png$", full.names = FALSE)
# Check the number of images in the folder
num_images <- length(image_files)
# Display images based on the number of images in the folder
if (num_images == 1) {
# Load and display the single image
img <- readPNG(image_files[1])
plot(1:2, type='n', xlab="", ylab="", axes=FALSE, main=image_titles[1])  # Create an empty plot with a title
rasterImage(img, 1, 1, 2, 2)  # Display the image
} else if (num_images == 2) {
# Set up the plot area for two side-by-side images
par(mfrow = c(1, 2))  # Divide the plot area into 1 row and 2 columns
# Load and display the first image
img1 <- readPNG(image_files[1])
plot(1:2, type='n', xlab="", ylab="", axes=FALSE, main=image_titles[1])
rasterImage(img1, 1, 1, 2, 2)
# Load and display the second image
img2 <- readPNG(image_files[2])
plot(1:2, type='n', xlab="", ylab="", axes=FALSE, main=image_titles[2])
rasterImage(img2, 1, 1, 2, 2)
# Reset the plotting area to the default
par(mfrow = c(1, 1))
} else {
message("No images found or more than two images present in the folder.")
}
}
display_images("an")
# ### create plot for word
# create_plot_for_word <- function(word_input) {
#   word_data <- sbc %>%
#     filter(tolower(text) == tolower(word_input)) %>%
#     filter(!is.na(place))
#
#   # Prepare the grouping data
#   group <- word_data[c("length_minus_1", "place_minus_1")]
#   colnames(group) <- c("nb_length", "nb_place")
#
#   # Group the data and calculate counts
#   group <- group %>% group_by(nb_length, nb_place) %>% count()
#
#   # Aggregate to get total counts per nb_length
#   result <- aggregate(n ~ nb_length, data = group, FUN = sum)
#
#   # Merge group data with the result to create the final dataset
#   result <- merge(group, result, by = "nb_length")
#
#   # Create the labels map for custom labeller
#   labels_map <- result %>%
#     group_by(nb_length) %>%
#     summarise(n.y = unique(n.y)) %>%
#     deframe()
#
#   # Custom labeller function
#   custom_labeller <- function(nb_length) {
#     sapply(nb_length, function(x) paste(x, "size:", labels_map[x]))
#   }
#
#   # Create the plot
#   p <- ggplot(data = result, aes(x = nb_place, y = n.x / n.y)) +
#     geom_point(alpha = 0.015, position = "jitter") +  # Scatter plot with jitter and transparency
#
#     # Facet by nb_length using the custom labeller
#     facet_wrap("nb_length", labeller = as_labeller(custom_labeller)) +
#
#     # Add red points for nb_length <= 20
#     geom_point(data = result %>% filter(nb_length <= 20), aes(x = nb_place, y = n.x / n.y), color = "red") +
#
#     # Add title and labels
#     labs(title = word_input) +
#     theme(plot.title = element_text(hjust = 0.5)) +  # Center the title
#     xlab("Place") +
#     ylab("Count Proportion")
#
#   # Return both the result and the plot as a list
#   return(list(result = result, plot = p))
# }
#
# #### who
# result_and_plot_who <- create_plot_for_word("who")
# print(result_and_plot_who$plot)
#
# dip_test_who <- run_dip_test_for_length(result_and_plot_who$result)
# print(dip_test_who)
# print(dip_test_who %>% filter(p_value < 0.05))
#
# skewness_kurtosis_who <- run_skewness_kurtosis_for_length(result_and_plot_who$result)
# print(skewness_kurtosis_who)
View(final_results)
filtered_results <- final_results %>% filter(model_type == "gen") %>%
select(word, back, theta_mean)
library(dplyr)
filtered_results <- final_results %>% filter(model_type == "gen") %>%
select(word, back, theta_mean)
head(filter)
head(filtered_results)
filtered_results <- final_results %>% filter(model_type == "gen") %>%
select(word, back, theta_mean, mu_mean, phi_mean, lambda_mean)
head(filtered_results)
dim(filtered_results)
setdiff(sbc_top200, final_results$word)
install.packages("clustMixType")
library(clustMixType)
params <- filtered_results[,-1]
set.seed(1)
kproto_result <- kproto(params, k = 3)
typeof(filtered_results)
typeof(filtered_results$word)
typeof(filtered_results$back)
typeof(filtered_results$theta_mean)
filtered_results$back <- as.factor(filtered_results$back)
params <- filtered_results[,-1]
set.seed(1)
kproto_result <- kproto(params, k = 5)
clusters <- kproto_result$cluster
filtered_results$cluster <- clusters
View(filtered_results)
wss <- numeric() for (k in 1:10) {  # Test k from 1 to 10 clusters (adjust the range as needed)
wss <- numeric()
for (k in 1:10) {  # Test k from 1 to 10 clusters (adjust the range as needed)
kproto_result <-kproto(params, k)
wss[k] <- kproto_result$tot.withinss  # Total within-cluster sum of squares
} # Plot the WSS vs. number of clusters
plot(1:10, wss, type = "b", pch = 19, frame = FALSE, xlab = "Number of clusters (k)", ylab = "Total within-cluster sum of squares (WSS)")
params <- filtered_results[,-1]
set.seed(1)
kproto_result <- kproto(params, k = 4)
clusters <- kproto_result$cluster
filtered_results$cluster <- clusters
filtered_results <- final_results %>% filter(model_type == "gen") %>%
select(word, back, theta_mean, theta_sd, mu_mean, mu_sd, phi_mean, phi_sd, lambda_mean, lambda_sd)
filtered_results$back <- as.factor(filtered_results$back)
params <- filtered_results[,-1]
set.seed(1)
kproto_result <- kproto(params, k = 4)
clusters <- kproto_result$cluster
filtered_results$cluster <- clusters
params <- filtered_results[,-1]
set.seed(1)
kproto_result <- kproto(params, k = 8)
clusters <- kproto_result$cluster
filtered_results$cluster <- clusters
params <- filtered_results[,-1]
set.seed(1)
kproto_result <- kproto(params, k = 22)
clusters <- kproto_result$cluster
filtered_results$cluster <- clusters
filtered_results <- final_results %>% filter(model_type == "gen") %>%
select(word, back, theta_mean, mu_mean, phi_mean, lambda_mean)
filtered_results$back <- as.factor(filtered_results$back)
params <- filtered_results[,-1]
set.seed(1)
kproto_result <- kproto(params, k = 22)
clusters <- kproto_result$cluster
filtered_results$cluster <- clusters
filtered_results <- final_results %>% filter(model_type == "gen") %>%
select(word, back, theta_mean, theta_sd, mu_mean, mu_sd, phi_mean, phi_sd, lambda_mean, lambda_sd)
filtered_results$back <- as.factor(filtered_results$back)
params <- filtered_results[,-1]
set.seed(1)
kproto_result <- kproto(params, k = 22)
clusters <- kproto_result$cluster
filtered_results$cluster <- clusters
library(cluster, lib.loc = "C:/Program Files/R/R-4.3.2/library")
head(filtered_results)
filtered_results$back_numeric <- as.numeric(filtered_results$back)
# Step 2: Select numeric columns for clustering (exclude 'word' and 'cluster')
numeric_data <- filtered_results[ , c("back_numeric", "theta_mean", "theta_sd", "mu_mean", "mu_sd", "phi_mean", "phi_sd", "lambda_mean", "lambda_sd")]
# Step 3: Calculate the distance matrix (using Euclidean distance)
dist_matrix <- dist(numeric_data, method = "euclidean")
# Step 4: Perform hierarchical clustering using complete linkage
hc <- hclust(dist_matrix, method = "complete")
# Step 5: Plot the dendrogram
plot(hc, labels = FALSE, main = "Dendrogram", sub = "", xlab = "Observations", ylab = "Height")
# Step 6: Cut the tree to form clusters (choose the number of clusters, e.g., 5)
clusters <- cutree(hc, k = 5)
# Step 7: Add the cluster assignments back to the original data
filtered_results$cluster_hc <- clusters
# Step 8: View the data with the new cluster assignments (excluding 'word' and original 'cluster')
head(filtered_results[ , c("back", "theta_mean", "theta_sd", "mu_mean", "mu_sd", "phi_mean", "phi_sd", "lambda_mean", "lambda_sd", "cluster_hc")])
cluster_hc
clusters <- cutree(hc, k = 22)
# Step 7: Add the cluster assignments back to the original data
filtered_results$cluster_hc <- clusters
# Step 2: Select numeric columns for clustering (exclude 'word' and 'cluster')
numeric_data <- filtered_results[ , c("back_numeric", "theta_mean", "mu_mean", "phi_mean", "lambda_mean")]
# Step 3: Calculate the distance matrix (using Euclidean distance)
dist_matrix <- dist(numeric_data, method = "euclidean")
# Step 4: Perform hierarchical clustering using complete linkage
hc <- hclust(dist_matrix, method = "complete")
# Step 5: Plot the dendrogram
plot(hc, labels = FALSE, main = "Dendrogram", sub = "", xlab = "Observations", ylab = "Height")
# Step 6: Cut the tree to form clusters (choose the number of clusters, e.g., 5)
clusters <- cutree(hc, k = 22)
# Step 7: Add the cluster assignments back to the original data
filtered_results$cluster_hc <- clusters
